"""
Fused DeepSeek-MoE Triton Kernel Implementation.

This module contains the core Triton kernel that performs the "DeepSeek Fusion":
- Load input X ONCE into SRAM via Virtual Gathering
- Compute both Routed Expert GEMM and Shared Expert GEMM
- Accumulate in FP32 registers, write back in BF16

The key optimization is that we eliminate the second HBM read for the shared
expert by keeping X in L1 cache while computing both paths.

Memory Bandwidth Analysis:
-------------------------
Naive approach:
  - Load X for routed: N * H * 2 bytes (BF16)
  - Load X for shared: N * H * 2 bytes (BF16)  
  - Total: 2 * N * H * 2 bytes

Fused approach:
  - Load X once: N * H * 2 bytes (BF16)
  - Total: N * H * 2 bytes

Theoretical speedup: ~2x on memory-bound workloads (large batch sizes)
"""

import triton
import triton.language as tl
import torch


@triton.jit
def fused_moe_forward_kernel(
    # ==========================================================================
    # Pointer Arguments (Base addresses of tensors in HBM)
    # ==========================================================================
    X_ptr,              # Input activations: [N, H] in BF16
    W_routed_ptr,       # Routed expert weights: [E, H, D] in BF16
    W_shared_ptr,       # Shared expert weights: [H, D] in BF16
    Y_ptr,              # Output buffer: [total_assignments, D] in BF16 (sorted order)
    
    # Routing metadata (generated by compute_routing_metadata)
    sorted_token_indices_ptr,   # [total_assignments] int32 - Row indices into X
    expert_block_offsets_ptr,   # [E+1] int32 - Cumsum of blocks per expert
    expert_token_offsets_ptr,   # [E+1] int32 - Cumsum of tokens per expert
    
    # ==========================================================================
    # Dimension Arguments
    # ==========================================================================
    N,                  # Total tokens (batch_size * seq_len)
    H,                  # Hidden dimension (input)
    D,                  # Intermediate dimension (output)
    num_experts: tl.constexpr,  # Number of routed experts
    
    # ==========================================================================
    # Stride Arguments (for pointer arithmetic)
    # ==========================================================================
    # Strides are in ELEMENTS, not bytes. To get byte offset: stride * sizeof(dtype)
    #
    # For X of shape [N, H]:
    #   X[i, j] is at X_ptr + i * stride_xn + j * stride_xh
    #   With row-major layout: stride_xn = H, stride_xh = 1
    stride_xn,          # Stride to move one row in X (= H for contiguous)
    stride_xh,          # Stride to move one column in X (= 1 for contiguous)
    
    # For W_routed of shape [E, H, D]:
    #   W_routed[e, h, d] is at W_routed_ptr + e * stride_we + h * stride_wh + d * stride_wd
    stride_we,          # Stride between experts (= H * D)
    stride_wh,          # Stride along hidden dim (= D)
    stride_wd,          # Stride along output dim (= 1)
    
    # For W_shared of shape [H, D]:
    #   W_shared[h, d] is at W_shared_ptr + h * stride_sh + d * stride_sd
    stride_sh,          # Stride along hidden dim (= D)
    stride_sd,          # Stride along output dim (= 1)
    
    # For Y of shape [total_assignments, D]:
    stride_yn,          # Stride to move one row in Y
    stride_yd,          # Stride along output dim
    
    # ==========================================================================
    # Block Size Meta-Parameters (compile-time constants)
    # ==========================================================================
    BLOCK_M: tl.constexpr,  # Tokens per block (e.g., 32)
    BLOCK_N: tl.constexpr,  # Output dimension tile (e.g., 64)
    BLOCK_K: tl.constexpr,  # Hidden dimension tile for K-loop (e.g., 32)
):
    """
    Fused MoE Forward Kernel with Virtual Gathering.
    
    Execution Model:
    ---------------
    - Grid: 1D grid of total_blocks (from get_grid_config)
    - Each block processes BLOCK_M tokens for ONE specific expert
    - Block identifies its expert via binary search on expert_block_offsets
    
    Memory Access Pattern:
    ---------------------
    1. Load sorted_token_indices to get physical row positions in X
    2. Use indirect addressing to gather X rows (Virtual Gather)
    3. Load routed weights for this block's expert
    4. Load shared weights (same for all blocks)
    5. Fused GEMM: acc = X @ W_routed + X @ W_shared
    6. Store to sorted output buffer
    """
    
    # ==========================================================================
    # STEP 1: Identify This Block's Expert and Token Range
    # ==========================================================================
    # pid = program ID (which block we are in the 1D grid)
    pid = tl.program_id(0)
    
    # Binary search to find which expert this block belongs to
    # We need to find the largest expert_id such that:
    #   expert_block_offsets[expert_id] <= pid
    #
    # Implementation: Linear scan (simpler than binary search in Triton)
    # For production with E=64, consider implementing binary search
    expert_id = 0
    for e in range(num_experts):
        # Load the block offset for expert e+1
        next_offset = tl.load(expert_block_offsets_ptr + e + 1)
        # If pid >= next_offset, we haven't reached our expert yet
        expert_id = tl.where(pid >= next_offset, e + 1, expert_id)
    
    # Now expert_id tells us which expert we're computing
    # Get the starting block index for this expert
    expert_block_start = tl.load(expert_block_offsets_ptr + expert_id)
    
    # Local block index within this expert's workload
    # If expert starts at block 10 and pid=12, local_block_idx = 2
    local_block_idx = pid - expert_block_start
    
    # Get the starting token index for this expert in sorted_token_indices
    expert_token_start = tl.load(expert_token_offsets_ptr + expert_id)
    
    # How many tokens does this expert have in total?
    expert_token_end = tl.load(expert_token_offsets_ptr + expert_id + 1)
    num_tokens_for_expert = expert_token_end - expert_token_start
    
    # This block handles tokens [block_token_start, block_token_start + BLOCK_M)
    # within the expert's sorted token list
    block_token_start = local_block_idx * BLOCK_M
    
    # ==========================================================================
    # STEP 2: Setup Token Index Offsets for Virtual Gathering
    # ==========================================================================
    # offs_m: [0, 1, 2, ..., BLOCK_M-1] - Local token indices within this block
    offs_m = tl.arange(0, BLOCK_M)
    
    # Global position in sorted_token_indices array:
    # = expert_token_start + block_token_start + offs_m
    #
    # Example: Expert 2 tokens start at index 500 in sorted array
    #          This is block 1 (second block) for expert 2
    #          block_token_start = 1 * 32 = 32
    #          We read sorted_token_indices[500 + 32 + 0:32] = indices 532-563
    sorted_idx_positions = expert_token_start + block_token_start + offs_m
    
    # Mask to handle the last block which may be partially filled
    # If expert has 100 tokens and BLOCK_M=32:
    # - Block 0: tokens 0-31 (valid)
    # - Block 1: tokens 32-63 (valid)
    # - Block 2: tokens 64-95 (valid)
    # - Block 3: tokens 96-99 (only 4 valid, mask out 100-127)
    token_mask = (block_token_start + offs_m) < num_tokens_for_expert
    
    # ==========================================================================
    # STEP 3: Load Token Indices for Virtual Gathering
    # ==========================================================================
    # This is the KEY step that enables zero-copy routing!
    # 
    # sorted_token_indices[sorted_idx_positions] tells us:
    # "For this expert, the k-th token to process is actually row X[idx] in the input"
    #
    # Example: sorted_token_indices might contain [5, 12, 100, 3, ...]
    # Meaning: first token for this expert is X[5], second is X[12], etc.
    #
    # This is the "Metadata Map" generated by compute_routing_metadata
    row_indices = tl.load(
        sorted_token_indices_ptr + sorted_idx_positions,
        mask=token_mask,
        other=0,  # Load 0 for invalid positions (will be masked in output anyway)
    )
    
    # ==========================================================================
    # STEP 4: Setup Weight Pointers
    # ==========================================================================
    # Routed weights: W_routed[expert_id, :, :] 
    # Base pointer for this expert's weight matrix:
    #   W_routed_ptr + expert_id * stride_we
    # This points to a [H, D] matrix for this specific expert
    W_routed_expert_ptr = W_routed_ptr + expert_id * stride_we
    
    # Shared weights: W_shared[:, :] - same for all blocks
    # Just use W_shared_ptr directly (no expert offset)
    
    # ==========================================================================
    # STEP 5: Initialize Output Accumulator
    # ==========================================================================
    # We tile the output dimension D into chunks of BLOCK_N
    # Loop over output tiles and accumulate results
    
    # offs_n: [0, 1, 2, ..., BLOCK_N-1] - Output dimension indices
    offs_n = tl.arange(0, BLOCK_N)
    
    # Number of output tiles
    num_output_tiles = tl.cdiv(D, BLOCK_N)
    
    # Loop over output dimension tiles
    for out_tile_idx in range(num_output_tiles):
        # Current output column range: [out_start, out_start + BLOCK_N)
        out_start = out_tile_idx * BLOCK_N
        out_cols = out_start + offs_n
        
        # Mask for valid output columns (handle D not divisible by BLOCK_N)
        out_mask = out_cols < D
        
        # ======================================================================
        # Initialize FP32 accumulator for this output tile
        # Shape: [BLOCK_M, BLOCK_N] - one value per (token, output_dim) pair
        # ======================================================================
        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
        
        # ======================================================================
        # STEP 6: The K-Loop - Fused GEMM Computation
        # ======================================================================
        # We iterate over the hidden dimension H in chunks of BLOCK_K
        # For each chunk:
        #   1. Load X tile: [BLOCK_M, BLOCK_K] (Virtual Gather - ONCE)
        #   2. Load W_routed tile: [BLOCK_K, BLOCK_N]
        #   3. Load W_shared tile: [BLOCK_K, BLOCK_N]
        #   4. acc += X_tile @ W_routed_tile (Routed path)
        #   5. acc += X_tile @ W_shared_tile (Shared path - THE FUSION)
        #
        # KEY INSIGHT: X_tile is loaded ONCE but used TWICE!
        # This is the core bandwidth saving of our fused kernel.
        
        num_k_tiles = tl.cdiv(H, BLOCK_K)
        
        for k_tile_idx in range(num_k_tiles):
            k_start = k_tile_idx * BLOCK_K
            offs_k = k_start + tl.arange(0, BLOCK_K)
            
            # Mask for valid K positions
            k_mask = offs_k < H
            
            # ==================================================================
            # STEP 6a: Virtual Gather - Load X Tile from Scattered Positions
            # ==================================================================
            # This is where indirect addressing happens!
            #
            # Normally, to load X[i, k], we'd compute: X_ptr + i * stride_xn + k
            # But our tokens are SCATTERED - token indices come from row_indices
            #
            # For each token m in [0, BLOCK_M):
            #   actual_row = row_indices[m]  (e.g., 5, 12, 100, ...)
            #   X[actual_row, k] is at: X_ptr + actual_row * stride_xn + k
            #
            # Pointer calculation:
            #   X_ptrs[m, k] = X_ptr + row_indices[m] * stride_xn + offs_k[k] * stride_xh
            #
            # In Triton, we use broadcasting:
            #   row_indices[:, None] broadcasts to [BLOCK_M, BLOCK_K]
            #   offs_k[None, :] broadcasts to [BLOCK_M, BLOCK_K]
            
            # Compute pointer matrix: [BLOCK_M, BLOCK_K]
            X_ptrs = (
                X_ptr 
                + row_indices[:, None] * stride_xn  # Row offset (scattered!)
                + offs_k[None, :] * stride_xh       # Column offset (contiguous)
            )
            
            # Load X tile with masking for both dimensions
            # - token_mask: invalid tokens at block boundary
            # - k_mask: invalid K positions at hidden dim boundary
            X_tile = tl.load(
                X_ptrs,
                mask=token_mask[:, None] & k_mask[None, :],
                other=0.0,
            ).to(tl.float32)  # Cast to FP32 for accumulation
            
            # ==================================================================
            # STEP 6b: Load Routed Weight Tile
            # ==================================================================
            # W_routed[expert_id, k, out] where k in offs_k, out in out_cols
            #
            # Pointer: W_routed_expert_ptr + k * stride_wh + out * stride_wd
            #
            # Shape: [BLOCK_K, BLOCK_N]
            W_routed_ptrs = (
                W_routed_expert_ptr
                + offs_k[:, None] * stride_wh      # K dimension
                + out_cols[None, :] * stride_wd    # Output dimension
            )
            
            W_routed_tile = tl.load(
                W_routed_ptrs,
                mask=k_mask[:, None] & out_mask[None, :],
                other=0.0,
            ).to(tl.float32)
            
            # ==================================================================
            # STEP 6c: Load Shared Weight Tile
            # ==================================================================
            # W_shared[k, out] - same weights for ALL tokens (broadcast)
            #
            # Pointer: W_shared_ptr + k * stride_sh + out * stride_sd
            #
            # Shape: [BLOCK_K, BLOCK_N]
            W_shared_ptrs = (
                W_shared_ptr
                + offs_k[:, None] * stride_sh      # K dimension
                + out_cols[None, :] * stride_sd    # Output dimension
            )
            
            W_shared_tile = tl.load(
                W_shared_ptrs,
                mask=k_mask[:, None] & out_mask[None, :],
                other=0.0,
            ).to(tl.float32)
            
            # ==================================================================
            # STEP 6d: Fused Accumulation - THE CORE OPTIMIZATION
            # ==================================================================
            # X_tile is [BLOCK_M, BLOCK_K]
            # W_routed_tile is [BLOCK_K, BLOCK_N]
            # W_shared_tile is [BLOCK_K, BLOCK_N]
            #
            # acc += X_tile @ W_routed_tile  (Routed expert contribution)
            # acc += X_tile @ W_shared_tile  (Shared expert contribution)
            #
            # CRITICALLY: X_tile is still in registers/L1 cache!
            # We perform TWO dot products for ONE load of X.
            # This is the "Register-Level Fusion" that saves bandwidth.
            
            # Routed path
            acc += tl.dot(X_tile, W_routed_tile)
            
            # Shared path - X_tile reused without reload!
            acc += tl.dot(X_tile, W_shared_tile)
        
        # ======================================================================
        # STEP 7: Store Results to Sorted Output Buffer
        # ======================================================================
        # Output is written in SORTED order (not original token order)
        # Y[sorted_idx_positions, out_cols] = acc
        #
        # PyTorch will handle the scatter back to original order
        #
        # Pointer calculation:
        #   Y_ptrs[m, n] = Y_ptr + sorted_idx_positions[m] * stride_yn + out_cols[n] * stride_yd
        
        Y_ptrs = (
            Y_ptr
            + sorted_idx_positions[:, None] * stride_yn  # Row in sorted order
            + out_cols[None, :] * stride_yd              # Output column
        )
        
        # Cast back to BF16 for storage
        acc_bf16 = acc.to(tl.bfloat16)
        
        # Store with masking
        tl.store(
            Y_ptrs,
            acc_bf16,
            mask=token_mask[:, None] & out_mask[None, :],
        )


def fused_moe_forward(
    X: torch.Tensor,                    # [N, H] BF16
    W_routed: torch.Tensor,             # [E, H, D] BF16
    W_shared: torch.Tensor,             # [H, D] BF16
    sorted_token_indices: torch.Tensor, # [total_assignments] int32
    expert_block_offsets: torch.Tensor, # [E+1] int32
    expert_token_offsets: torch.Tensor, # [E+1] int32
    total_blocks: int,
    num_experts: int,
    BLOCK_M: int = 32,
    BLOCK_N: int = 64,
    BLOCK_K: int = 32,
) -> torch.Tensor:
    """
    Python wrapper for the fused MoE forward kernel.
    
    This function:
    1. Allocates the output buffer
    2. Computes strides for all tensors
    3. Launches the Triton kernel with appropriate grid
    
    Args:
        X: Input activations [N, H] in BF16
        W_routed: Routed expert weights [E, H, D] in BF16
        W_shared: Shared expert weights [H, D] in BF16
        sorted_token_indices: Metadata map for virtual gathering
        expert_block_offsets: Grid topology for block->expert mapping
        expert_token_offsets: Grid topology for expert->token mapping
        total_blocks: Total number of blocks to launch
        num_experts: Number of routed experts
        BLOCK_M, BLOCK_N, BLOCK_K: Tile sizes
    
    Returns:
        Y: Output activations [total_assignments, D] in BF16 (sorted order)
    """
    N, H = X.shape
    E, _, D = W_routed.shape
    total_assignments = sorted_token_indices.shape[0]
    
    # Allocate output buffer in sorted order
    Y = torch.empty(
        (total_assignments, D),
        dtype=torch.bfloat16,
        device=X.device,
    )
    
    # Launch kernel
    grid = (total_blocks,)
    
    fused_moe_forward_kernel[grid](
        # Pointers
        X, W_routed, W_shared, Y,
        sorted_token_indices, expert_block_offsets, expert_token_offsets,
        # Dimensions
        N, H, D, num_experts,
        # Strides for X [N, H]
        X.stride(0), X.stride(1),
        # Strides for W_routed [E, H, D]
        W_routed.stride(0), W_routed.stride(1), W_routed.stride(2),
        # Strides for W_shared [H, D]
        W_shared.stride(0), W_shared.stride(1),
        # Strides for Y [total_assignments, D]
        Y.stride(0), Y.stride(1),
        # Block sizes
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    
    return Y
